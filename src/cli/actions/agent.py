"""
Agent-based question generation and scoring CLI actions.

REQ: REQ-CLI-Agent-1, REQ-CLI-Agent-2
"""

import asyncio
import json
import logging

from rich.table import Table

from src.agent.llm_agent import GenerateQuestionsRequest, ItemGenAgent
from src.cli.context import CLIContext

logger = logging.getLogger(__name__)


def agent_help(context: CLIContext, *args: str) -> None:
    """
    Display help for agent command group.

    Shows available subcommands:
    - generate-questions: Question generation workflow (Mode 1)
    - score-answer: Single answer scoring (Mode 2)
    - batch-score: Parallel batch scoring (Mode 2)
    - tools: Individual tool debugging interface

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (ignored).

    """
    context.console.print()
    context.console.print(
        "[bold cyan]â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•‘  agent - Agent-based question generation and scoring                          â•‘[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]"
    )
    context.console.print()

    table = Table(show_header=False, box=None, padding=(0, 2))

    # Add subcommands
    subcommands = [
        ("agent generate-questions", "[dim]ğŸ“ ë¬¸í•­ ìƒì„± (Tool 1-5 ì²´ì¸)[/dim]"),
        ("agent score-answer", "[dim]ğŸ“‹ ë‹µë³€ ì±„ì  (Tool 6)[/dim]"),
        ("agent batch-score", "[dim]ğŸ“Š ë°°ì¹˜ ì±„ì  (ë³µìˆ˜ ë‹µë³€, ë³‘ë ¬)[/dim]"),
        ("agent tools", "[dim]ğŸ”§ ê°œë³„ Tool ë””ë²„ê¹…[/dim]"),
    ]

    for cmd, desc in subcommands:
        table.add_row(cmd, desc)

    context.console.print(table)
    context.console.print()
    context.console.print("[bold yellow]ğŸ’¡ íŒ:[/bold yellow] 'agent tools --help'ë¡œ ë””ë²„ê¹… ë„êµ¬ ë³´ê¸°")
    context.console.print()


def generate_questions(context: CLIContext, *args: str) -> None:
    """
    Generate high-quality questions using ItemGenAgent (Mode 1).

    Workflow: Calls Tool 1-5 in sequence to:
    1. Get user profile (Tool 1)
    2. Search question templates (Tool 2)
    3. Get difficulty keywords (Tool 3)
    4. Generate and validate questions (Tool 4)
    5. Save validated questions (Tool 5)

    Args:
        context: CLI context with console and logger.
        *args: Parsed arguments (--survey-id, --round, --prev-answers).

    REQ: REQ-CLI-Agent-2

    """
    # Parse arguments
    survey_id = None
    round_idx = 1
    prev_answers_json = None

    i = 0
    while i < len(args):
        arg = args[i]
        if arg == "--survey-id" and i + 1 < len(args):
            survey_id = args[i + 1]
            i += 2
        elif arg == "--round" and i + 1 < len(args):
            try:
                round_idx = int(args[i + 1])
                i += 2
            except ValueError:
                context.console.print(f"[bold red]âŒ Error:[/bold red] --round must be integer (got: {args[i + 1]})")
                return
        elif arg == "--prev-answers" and i + 1 < len(args):
            prev_answers_json = args[i + 1]
            i += 2
        elif arg == "--help":
            _print_generate_questions_help(context)
            return
        else:
            i += 1

    # Validate required survey-id
    if not survey_id:
        context.console.print("[bold red]âŒ Error:[/bold red] --survey-id is required")
        _print_generate_questions_help(context)
        return

    # Validate round
    if round_idx not in (1, 2):
        context.console.print(f"[bold red]âŒ Error:[/bold red] --round must be 1 or 2 (got: {round_idx})")
        return

    # Parse prev-answers if provided
    prev_answers = None
    if prev_answers_json:
        if round_idx != 2:
            context.console.print("[bold yellow]âš ï¸  Warning:[/bold yellow] --prev-answers only used in Round 2")
        try:
            prev_answers = json.loads(prev_answers_json)
            if not isinstance(prev_answers, list):
                context.console.print("[bold red]âŒ Error:[/bold red] --prev-answers must be JSON array")
                return
        except json.JSONDecodeError as e:
            context.console.print(f"[bold red]âŒ Error:[/bold red] Invalid JSON in --prev-answers: {e}")
            return

    # Initialize agent
    context.console.print("ğŸš€ Initializing Agent... (GEMINI_API_KEY required)")
    try:
        agent = ItemGenAgent()
    except Exception as e:
        context.console.print("[bold red]âŒ Error:[/bold red] Agent initialization failed")
        context.console.print(f"[dim]Reason: {e}[/dim]")
        return

    context.console.print("âœ… Agent initialized")

    # Create request
    context.console.print()
    context.console.print("ğŸ“ Generating questions...")
    context.console.print(f"   survey_id={survey_id}, round={round_idx}")

    request = GenerateQuestionsRequest(survey_id=survey_id, round_idx=round_idx, prev_answers=prev_answers)

    # Execute agent (async)
    try:
        response = asyncio.run(agent.generate_questions(request))
    except Exception as e:
        context.console.print()
        context.console.print("[bold red]âŒ Error:[/bold red] Question generation failed")
        context.console.print(f"[dim]Reason: {e}[/dim]")
        return

    # Display results
    context.console.print()
    context.console.print("âœ… Generation Complete")
    context.console.print(f"   round_id: {response.round_id}")
    context.console.print(f"   items generated: {len(response.items)}, failed: {response.failed_count}")
    context.console.print(f"   agent_steps: {response.agent_steps}")
    context.console.print()

    # Display table
    table = Table(title="Generated Items", show_header=True, header_style="bold cyan")
    table.add_column("ID", style="dim")
    table.add_column("Type")
    table.add_column("Difficulty", justify="right")
    table.add_column("Validation", justify="right")

    for item in response.items:
        item_id_short = item.id[:12] + "..." if len(item.id) > 12 else item.id
        table.add_row(
            item_id_short,
            item.type,
            str(item.difficulty),
            f"{item.validation_score:.2f}",
        )

    context.console.print("ğŸ“‹ Generated Items:")
    context.console.print(table)
    context.console.print()

    # Display first item details
    if response.items:
        first_item = response.items[0]
        context.console.print("ğŸ“„ First Item Details:")
        context.console.print(f"   Stem: {first_item.stem}")
        context.console.print(f"   Answer Schema: {first_item.answer_schema.type}")
        if first_item.answer_schema.keywords:
            keywords_str = ", ".join(first_item.answer_schema.keywords)
            context.console.print(f"   Keywords: [{keywords_str}]")
        context.console.print()


def score_answer(context: CLIContext, *args: str) -> None:
    """
    Score a single answer using Tool 6 with explanation generation.

    Supports multiple question types (MC, short answer, true/false) and returns
    detailed scoring with LLM-generated explanation.

    Args:
        context: CLI context with console and logger.
        *args: Parsed arguments (--question-id, --question, --answer-type,
                                 --user-answer, --correct-answer, --context).

    REQ: REQ-CLI-Agent-3

    """
    # Parse arguments
    question_id = None
    question_stem = None
    answer_type = None
    user_answer = None
    correct_answer = None
    context_str = None

    i = 0
    while i < len(args):
        arg = args[i]
        if arg == "--question-id" and i + 1 < len(args):
            question_id = args[i + 1]
            i += 2
        elif arg == "--question" and i + 1 < len(args):
            question_stem = args[i + 1]
            i += 2
        elif arg == "--answer-type" and i + 1 < len(args):
            answer_type = args[i + 1]
            i += 2
        elif arg == "--user-answer" and i + 1 < len(args):
            user_answer = args[i + 1]
            i += 2
        elif arg == "--correct-answer" and i + 1 < len(args):
            correct_answer = args[i + 1]
            i += 2
        elif arg == "--context" and i + 1 < len(args):
            context_str = args[i + 1]
            i += 2
        elif arg == "--help":
            _print_score_answer_help(context)
            return
        else:
            i += 1

    # Validate required parameters
    if not question_id:
        context.console.print("[bold red]âŒ Error:[/bold red] --question-id is required")
        _print_score_answer_help(context)
        return

    if not question_stem:
        context.console.print("[bold red]âŒ Error:[/bold red] --question is required")
        _print_score_answer_help(context)
        return

    if not answer_type:
        context.console.print("[bold red]âŒ Error:[/bold red] --answer-type is required")
        _print_score_answer_help(context)
        return

    if not user_answer:
        context.console.print("[bold red]âŒ Error:[/bold red] --user-answer is required")
        _print_score_answer_help(context)
        return

    if not correct_answer:
        context.console.print("[bold red]âŒ Error:[/bold red] --correct-answer is required")
        _print_score_answer_help(context)
        return

    # Validate answer-type
    valid_types = ["multiple_choice", "short_answer", "true_false"]
    if answer_type not in valid_types:
        context.console.print(f"[bold red]âŒ Error:[/bold red] --answer-type must be one of: {', '.join(valid_types)}")
        return

    # Initialize agent
    context.console.print("ğŸš€ Initializing Agent... (GEMINI_API_KEY required)")
    try:
        agent = ItemGenAgent()
    except Exception as e:
        context.console.print("[bold red]âŒ Error:[/bold red] Agent initialization failed")
        context.console.print(f"[dim]Reason: {e}[/dim]")
        return

    context.console.print("âœ… Agent initialized")

    # Create request
    context.console.print()
    context.console.print("ğŸ¯ Scoring answer...")
    context.console.print(f"   question_id={question_id}, type={answer_type}")

    from src.agent.llm_agent import ScoreAnswerRequest

    request = ScoreAnswerRequest(
        item_id=question_id,
        question_stem=question_stem,
        question_type=answer_type,
        user_answer=user_answer,
        correct_answer=correct_answer,
        context=context_str,
    )

    # Execute agent (async)
    try:
        response = asyncio.run(agent.score_answer(request))
    except Exception as e:
        context.console.print()
        context.console.print("[bold red]âŒ Error:[/bold red] Answer scoring failed")
        context.console.print(f"[dim]Reason: {e}[/dim]")
        return

    # Display results
    context.console.print()
    context.console.print("âœ… Scoring Complete")
    context.console.print(f"   correct: {response.correct}")
    context.console.print(f"   score: {response.score}")
    if hasattr(response, "confidence") and response.confidence is not None:
        context.console.print(f"   confidence: {response.confidence:.2f}")
    context.console.print()

    # Display scoring result panel
    correctness_status = "âœ… CORRECT" if response.correct else "âŒ INCORRECT"
    panel_content = f"""
Question: {question_stem[:60]}...
User Answer: {user_answer}
Correct Answer: {correct_answer}
Score: {response.score}/100
Status: {correctness_status}"""

    from rich.panel import Panel

    context.console.print(Panel(panel_content, title="ğŸ“Š Scoring Result", style="cyan"))

    # Display explanation
    if response.explanation:
        context.console.print()
        context.console.print("ğŸ“ Explanation:")
        context.console.print(response.explanation)

    # Display matched keywords if available
    if hasattr(response, "keyword_matches") and response.keyword_matches:
        context.console.print()
        context.console.print(f"ğŸ’¡ Keywords Matched: {response.keyword_matches}")

    # Display confidence
    if hasattr(response, "confidence") and response.confidence is not None:
        context.console.print()
        context.console.print(f"ğŸ¯ Confidence: {response.confidence * 100:.0f}%")

    context.console.print()


def batch_score(context: CLIContext, *args: str) -> None:
    """
    Score multiple answers in parallel using Tool 6.

    Requires:
    - batch_file: JSON file with array of {question_id, answer} objects

    Supports parallel execution for improved performance.

    Args:
        context: CLI context with console and logger.
        *args: Arguments (batch_file path).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] batch-score implementation "
    msg1 += "pending (REQ-CLI-Agent-4)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-4 will implement: Parallel Tool 6 execution "
    msg2 += "(asyncio.gather)[/dim]"
    context.console.print(msg2)


def tools_help(context: CLIContext, *args: str) -> None:
    """
    Display help for tools debugging interface.

    Shows available tools:
    - t1: Get User Profile
    - t2: Search Question Templates
    - t3: Get Difficulty Keywords
    - t4: Validate Question Quality
    - t5: Save Generated Question
    - t6: Score & Generate Explanation

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (ignored).

    """
    context.console.print()
    context.console.print(
        "[bold cyan]â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•‘  agent tools - Tool debugging interface                                       â•‘[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]"
    )
    context.console.print()

    table = Table(show_header=False, box=None, padding=(0, 2))

    # Add tools
    tools = [
        ("agent tools t1", "[dim]ğŸ” Get User Profile (Tool 1)[/dim]"),
        ("agent tools t2", "[dim]ğŸ“š Search Question Templates (Tool 2)[/dim]"),
        ("agent tools t3", "[dim]ğŸ“Š Get Difficulty Keywords (Tool 3)[/dim]"),
        ("agent tools t4", "[dim]âœ… Validate Question Quality (Tool 4)[/dim]"),
        ("agent tools t5", "[dim]ğŸ’¾ Save Generated Question (Tool 5)[/dim]"),
        ("agent tools t6", "[dim]ğŸ¯ Score & Generate Explanation (Tool 6)[/dim]"),
    ]

    for cmd, desc in tools:
        table.add_row(cmd, desc)

    context.console.print(table)
    context.console.print()


def t1_get_user_profile(context: CLIContext, *args: str) -> None:
    """
    Tool 1: Get User Profile (debugging interface).

    Invokes FastMCP Tool 1: get_user_profile
    Returns user's skill level, experience, interests, job role, etc.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (user_id - reserved for future).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t1 (get_user_profile) "
    msg1 += "implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 1 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def t2_search_question_templates(context: CLIContext, *args: str) -> None:
    """
    Tool 2: Search Question Templates (debugging interface).

    Invokes FastMCP Tool 2: search_question_templates
    Returns templates matching interests, difficulty, and category.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (interests, difficulty, category).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t2 "
    msg1 += "(search_question_templates) implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 2 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def t3_get_difficulty_keywords(context: CLIContext, *args: str) -> None:
    """
    Tool 3: Get Difficulty Keywords (debugging interface).

    Invokes FastMCP Tool 3: get_difficulty_keywords
    Returns keywords and concepts for specified difficulty level.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (difficulty, category).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t3 "
    msg1 += "(get_difficulty_keywords) implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 3 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def t4_validate_question_quality(context: CLIContext, *args: str) -> None:
    """
    Tool 4: Validate Question Quality (debugging interface).

    Invokes FastMCP Tool 4: validate_question_quality
    Returns validation score and feedback for a question stem.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (question_stem, question_type).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t4 "
    msg1 += "(validate_question_quality) implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 4 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def t5_save_generated_question(context: CLIContext, *args: str) -> None:
    """
    Tool 5: Save Generated Question (debugging interface).

    Invokes FastMCP Tool 5: save_generated_question
    Saves a validated question with metadata to database.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (item_type, stem, difficulty, categories,
                round_id).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t5 "
    msg1 += "(save_generated_question) implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 5 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def t6_score_and_explain(context: CLIContext, *args: str) -> None:
    """
    Tool 6: Score Answer & Generate Explanation (debugging interface).

    Invokes FastMCP Tool 6: score_and_explain
    Scores an answer using LLM and generates detailed explanation.

    Args:
        context: CLI context with console and logger.
        *args: Additional arguments (question_id, answer, question_context).

    """
    msg1 = "[bold yellow]âš ï¸  Placeholder:[/bold yellow] t6 (score_and_explain) "
    msg1 += "implementation pending (REQ-CLI-Agent-5)"
    context.console.print(msg1)
    msg2 = "[dim]REQ-CLI-Agent-5 will implement: Direct FastMCP Tool 6 "
    msg2 += "invocation[/dim]"
    context.console.print(msg2)


def _print_generate_questions_help(context: CLIContext) -> None:
    """
    Display help for generate-questions command.

    Args:
        context: CLI context with console and logger.

    """
    context.console.print()
    context.console.print(
        "[bold cyan]â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•‘  agent generate-questions - Mode 1 Question Generation                        â•‘[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]"
    )
    context.console.print()
    context.console.print("[bold white]Usage:[/bold white]")
    context.console.print("  agent generate-questions --survey-id SURVEY_ID [--round 1|2] [--prev-answers JSON]")
    context.console.print()
    context.console.print("[bold white]Options:[/bold white]")
    context.console.print("  --survey-id TEXT         Survey ID (required)")
    context.console.print("  --round INTEGER          Round number: 1 (initial) or 2 (adaptive) [default: 1]")
    context.console.print("  --prev-answers TEXT      JSON array of previous answers (Round 2 only)")
    context.console.print('                           Format: \'[{"item_id":"q1","score":85}]\'')
    context.console.print("  --help                   Show this help message")
    context.console.print()
    context.console.print("[bold white]Examples:[/bold white]")
    context.console.print("  # Generate Round 1 questions")
    context.console.print("  agent generate-questions --survey-id survey_123")
    context.console.print()
    context.console.print("  # Generate Round 2 with adaptive difficulty")
    context.console.print(
        '  agent generate-questions --survey-id survey_123 --round 2 \'--prev-answers [{"item_id":"q1","score":85}]\''
    )
    context.console.print()


def _print_score_answer_help(context: CLIContext) -> None:
    """
    Display help for score-answer command.

    Args:
        context: CLI context with console and logger.

    """
    context.console.print()
    context.console.print(
        "[bold cyan]â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•‘  agent score-answer - Mode 2 Single Answer Scoring                           â•‘[/bold cyan]"
    )
    context.console.print(
        "[bold cyan]â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[/bold cyan]"
    )
    context.console.print()
    context.console.print("[bold white]Usage:[/bold white]")
    context.console.print("  agent score-answer --question-id ID --question STEM --answer-type TYPE")
    context.console.print("                     --user-answer ANSWER --correct-answer CORRECT [--context TEXT]")
    context.console.print()
    context.console.print("[bold white]Options:[/bold white]")
    context.console.print("  --question-id TEXT       Question ID (required)")
    context.console.print("  --question TEXT          Question stem (required)")
    context.console.print(
        "  --answer-type TEXT       Question type (required): multiple_choice, short_answer, true_false"
    )
    context.console.print("  --user-answer TEXT       User's answer (required)")
    context.console.print("  --correct-answer TEXT    Correct answer (required)")
    context.console.print("  --context TEXT           Additional context (optional)")
    context.console.print("  --help                   Show this help message")
    context.console.print()
    context.console.print("[bold white]Examples:[/bold white]")
    context.console.print("  # Score a multiple choice answer")
    context.console.print(
        "  agent score-answer --question-id q_001 --question 'What is X?' \\\n"
        "                     --answer-type multiple_choice --user-answer 'Option A' \\\n"
        "                     --correct-answer 'Option A'"
    )
    context.console.print()
    context.console.print("  # Score a short answer")
    context.console.print(
        "  agent score-answer --question-id q_002 --question 'Explain Y' \\\n"
        "                     --answer-type short_answer --user-answer 'My explanation' \\\n"
        "                     --correct-answer 'Complete explanation'"
    )
    context.console.print()
